{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Download GPT2 models "
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Install transformers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEAPIKhf6s7J"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use the transformers library to download LLM(large language models), using which we can download the GPT2 model for different languages.\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**AutoModelForCausalLM**: This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created with the *`from_pretrained()`* class method or the *`from_config()`* class method.\n",
    "\n",
    "**AutoTokenizer**:  This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the *`AutoTokenizer.from_pretrained()`* class method.\n",
    "\n",
    "**Pickle**: We use this library to output models."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import pickle"
   ],
   "metadata": {
    "id": "CD6iDIciycl2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To download our models, we need to replace `model_name` with the name of the model we want to download.\n",
    "\n",
    "The models we use are as follows:\n",
    "\n",
    "- `dbmdz/german-gpt2` (german)\n",
    "- `birgermoell/swedish-gpt` (swedish)\n",
    "- `ai-forever/rugpt3small_based_on_gpt2` (russian)\n",
    "- `gpt2` (english)\n",
    "- `flax-community/gpt-2-spanish` (spanish)\n",
    "- `ClassCat/gpt2-base-french` (french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bSkdDKzr6sKx"
   },
   "outputs": [],
   "source": [
    "toker = AutoTokenizer.from_pretrained(\"<model_name>\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"<model_name>\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To output models, we use the `Pickle` library and the `dump()` method.\n",
    "\n",
    "We save the models in the following path:\n",
    "\n",
    "- `/content/drive/MyDrive/models_predi_text/Language_model`\n",
    "- `/content/drive/MyDrive/models_predi_text/Language_tokenizer`\n",
    "\n",
    "Instead of the word `language` at the end of each path, we use the language name.\n",
    " \n",
    "### Names:\n",
    "\n",
    "- english\n",
    "- spanish\n",
    "- swedish\n",
    "- russian\n",
    "- french\n",
    "- german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jm7xpf7EB9Z4"
   },
   "outputs": [],
   "source": [
    "pickle.dump(model,open('/content/drive/MyDrive/models_predi_text/english_model.pkl','wb'))\n",
    "pickle.dump(toker,open('/content/drive/MyDrive/models_predi_text/english_tokenizer.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
